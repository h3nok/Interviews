<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Diffusion Models & Multimodal AI - AWS Interview Prep</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/reveal.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/theme/black.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/monokai.min.css">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide -->
            <section data-background-gradient="linear-gradient(45deg, #059669, #1e3a8a)">
                <h1>üé® Diffusion Models & Multimodal AI</h1>
                <p>Advanced generative models for the modern AI researcher</p>
                <div class="navigation-hint">
                    <p><a href="index.html" style="color: #ff6b35;">‚Üê Back to Main</a> | Press <kbd>‚Üí</kbd> to continue</p>
                </div>
            </section>

            <!-- Diffusion Models Section -->
            <section>
                <section data-background-color="#059669">
                    <h1>üåä Diffusion Models</h1>
                    <p>Probabilistic generative models that learn to reverse noise</p>
                </section>

                <section>
                    <h2>üß† Core Intuition</h2>
                    <div class="diffusion-process">
                        <h3>Forward Process (Noise Addition)</h3>
                        <div class="process-step">
                            <span>Clean Image</span> ‚Üí <span>+ Noise</span> ‚Üí <span>+ More Noise</span> ‚Üí <span>Pure Noise</span>
                        </div>
                        
                        <h3>Reverse Process (Denoising)</h3>
                        <div class="process-step reverse">
                            <span>Pure Noise</span> ‚Üí <span>- Noise</span> ‚Üí <span>- More Noise</span> ‚Üí <span>Clean Image</span>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üìê Mathematical Foundation</h2>
                    <div class="math-section">
                        <h3>Forward Process</h3>
                        <div class="formula">
                            <code>q(x<sub>t</sub>|x<sub>t-1</sub>) = N(x<sub>t</sub>; ‚àö(1-Œ≤<sub>t</sub>)x<sub>t-1</sub>, Œ≤<sub>t</sub>I)</code>
                        </div>
                        
                        <h3>Reverse Process</h3>
                        <div class="formula">
                            <code>p<sub>Œ∏</sub>(x<sub>t-1</sub>|x<sub>t</sub>) = N(x<sub>t-1</sub>; Œº<sub>Œ∏</sub>(x<sub>t</sub>,t), Œ£<sub>Œ∏</sub>(x<sub>t</sub>,t))</code>
                        </div>
                        
                        <h3>Training Objective (Simplified)</h3>
                        <div class="formula">
                            <code>L = E<sub>t,x<sub>0</sub>,Œµ</sub>[||Œµ - Œµ<sub>Œ∏</sub>(x<sub>t</sub>, t)||¬≤]</code>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üèóÔ∏è Architecture: U-Net</h2>
                    <div class="unet-components">
                        <div class="component">
                            <h3>Encoder (Downsampling)</h3>
                            <ul>
                                <li>ResNet blocks</li>
                                <li>Attention layers</li>
                                <li>Downsampling convolutions</li>
                            </ul>
                        </div>
                        <div class="component">
                            <h3>Bottleneck</h3>
                            <ul>
                                <li>Lowest resolution</li>
                                <li>Maximum feature depth</li>
                                <li>Global attention</li>
                            </ul>
                        </div>
                        <div class="component">
                            <h3>Decoder (Upsampling)</h3>
                            <ul>
                                <li>Skip connections</li>
                                <li>Upsampling layers</li>
                                <li>Feature combination</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üéØ Key Innovations</h2>
                    <div class="innovations-grid">
                        <div class="innovation">
                            <h3>DDPM (2020)</h3>
                            <p>Denoising Diffusion Probabilistic Models</p>
                            <span class="innovation-detail">Fixed noise schedule, U-Net architecture</span>
                        </div>
                        <div class="innovation">
                            <h3>DDIM (2021)</h3>
                            <p>Deterministic sampling</p>
                            <span class="innovation-detail">Faster generation, fewer steps</span>
                        </div>
                        <div class="innovation">
                            <h3>Classifier-Free Guidance</h3>
                            <p>Conditional generation without classifier</p>
                            <span class="innovation-detail">Better quality, easier training</span>
                        </div>
                        <div class="innovation">
                            <h3>Latent Diffusion (Stable Diffusion)</h3>
                            <p>Diffusion in latent space</p>
                            <span class="innovation-detail">Computational efficiency, high resolution</span>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üîß Sampling Methods</h2>
                    <div class="sampling-methods">
                        <div class="method">
                            <h3>DDPM Sampling</h3>
                            <ul>
                                <li>1000 steps (slow but high quality)</li>
                                <li>Stochastic process</li>
                                <li>Original formulation</li>
                            </ul>
                        </div>
                        <div class="method">
                            <h3>DDIM Sampling</h3>
                            <ul>
                                <li>50-100 steps (faster)</li>
                                <li>Deterministic process</li>
                                <li>Interpolatable latent space</li>
                            </ul>
                        </div>
                        <div class="method">
                            <h3>DPM-Solver</h3>
                            <ul>
                                <li>10-20 steps (very fast)</li>
                                <li>Higher-order solver</li>
                                <li>Maintains quality</li>
                            </ul>
                        </div>
                    </div>
                </section>
            </section>

            <!-- Multimodal AI Section -->
            <section>
                <section data-background-color="#7c3aed">
                    <h1>üé≠ Multimodal AI</h1>
                    <p>Bridging vision, language, and beyond</p>
                </section>

                <section>
                    <h2>üß© Modality Fusion Strategies</h2>
                    <div class="fusion-strategies">
                        <div class="strategy">
                            <h3>Early Fusion</h3>
                            <p>Combine features at input level</p>
                            <div class="pros-cons">
                                <div class="pros">‚úÖ Rich interaction</div>
                                <div class="cons">‚ùå Modality mismatch issues</div>
                            </div>
                        </div>
                        <div class="strategy">
                            <h3>Late Fusion</h3>
                            <p>Process separately, combine at end</p>
                            <div class="pros-cons">
                                <div class="pros">‚úÖ Modality-specific processing</div>
                                <div class="cons">‚ùå Limited cross-modal learning</div>
                            </div>
                        </div>
                        <div class="strategy">
                            <h3>Hybrid Fusion</h3>
                            <p>Multiple fusion points</p>
                            <div class="pros-cons">
                                <div class="pros">‚úÖ Best of both worlds</div>
                                <div class="cons">‚ùå Increased complexity</div>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üëÅÔ∏è Vision-Language Models</h2>
                    <div class="vl-models">
                        <div class="model">
                            <h3>CLIP</h3>
                            <p>Contrastive Language-Image Pre-training</p>
                            <ul>
                                <li>Dual encoder architecture</li>
                                <li>Contrastive learning</li>
                                <li>Zero-shot classification</li>
                            </ul>
                        </div>
                        <div class="model">
                            <h3>DALLE/DALLE-2</h3>
                            <p>Text-to-image generation</p>
                            <ul>
                                <li>Autoregressive ‚Üí Diffusion</li>
                                <li>CLIP guidance</li>
                                <li>High-quality synthesis</li>
                            </ul>
                        </div>
                        <div class="model">
                            <h3>Flamingo</h3>
                            <p>Few-shot learning across modalities</p>
                            <ul>
                                <li>Perceiver resampler</li>
                                <li>Cross-attention to vision</li>
                                <li>In-context learning</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üéØ Cross-Modal Attention</h2>
                    <div class="cross-attention-explanation">
                        <h3>How It Works</h3>
                        <div class="attention-flow">
                            <div class="modality">
                                <h4>Text (Query)</h4>
                                <p>"A red car"</p>
                            </div>
                            <div class="arrow">‚Üí</div>
                            <div class="attention-mechanism">
                                <h4>Cross-Attention</h4>
                                <p>Which image regions relate to "red" and "car"?</p>
                            </div>
                            <div class="arrow">‚Üí</div>
                            <div class="modality">
                                <h4>Vision (Key/Value)</h4>
                                <p>Image features</p>
                            </div>
                        </div>
                        <div class="formula">
                            <code>Attention(Q<sub>text</sub>, K<sub>vision</sub>, V<sub>vision</sub>)</code>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üìä Evaluation Challenges</h2>
                    <div class="evaluation-challenges">
                        <div class="challenge">
                            <h3>Alignment Metrics</h3>
                            <ul>
                                <li><strong>CLIP Score</strong>: Semantic similarity</li>
                                <li><strong>BLIP Score</strong>: Bidirectional evaluation</li>
                                <li><strong>Human evaluation</strong>: Gold standard</li>
                            </ul>
                        </div>
                        <div class="challenge">
                            <h3>Compositional Understanding</h3>
                            <ul>
                                <li><strong>Winoground</strong>: Compositional reasoning</li>
                                <li><strong>VQA datasets</strong>: Question answering</li>
                                <li><strong>Counting tasks</strong>: Precise understanding</li>
                            </ul>
                        </div>
                        <div class="challenge">
                            <h3>Robustness Testing</h3>
                            <ul>
                                <li><strong>Adversarial examples</strong>: Model failures</li>
                                <li><strong>Distribution shift</strong>: Domain adaptation</li>
                                <li><strong>Bias evaluation</strong>: Fairness metrics</li>
                            </ul>
                        </div>
                    </div>
                </section>
            </section>

            <!-- Advanced Topics -->
            <section>
                <section data-background-color="#dc2626">
                    <h1>üöÄ Advanced Topics</h1>
                    <p>Cutting-edge research directions</p>
                </section>

                <section>
                    <h2>üé® Advanced Diffusion Techniques</h2>
                    <div class="advanced-techniques">
                        <div class="technique">
                            <h3>ControlNet</h3>
                            <p>Spatial conditioning for diffusion models</p>
                            <ul>
                                <li>Canny edges, depth maps</li>
                                <li>Pose estimation</li>
                                <li>Fine-grained control</li>
                            </ul>
                        </div>
                        <div class="technique">
                            <h3>LoRA for Diffusion</h3>
                            <p>Efficient fine-tuning</p>
                            <ul>
                                <li>Low-rank adaptation</li>
                                <li>Style transfer</li>
                                <li>Concept learning</li>
                            </ul>
                        </div>
                        <div class="technique">
                            <h3>IP-Adapter</h3>
                            <p>Image prompt conditioning</p>
                            <ul>
                                <li>Image-to-image prompting</li>
                                <li>Style consistency</li>
                                <li>Reference-based generation</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üé≠ Multimodal Frontiers</h2>
                    <div class="frontiers-grid">
                        <div class="frontier">
                            <h3>Audio-Visual Models</h3>
                            <ul>
                                <li>Speech-to-video synthesis</li>
                                <li>Music visualization</li>
                                <li>Cross-modal retrieval</li>
                            </ul>
                        </div>
                        <div class="frontier">
                            <h3>3D Understanding</h3>
                            <ul>
                                <li>NeRF integration</li>
                                <li>3D scene understanding</li>
                                <li>Spatial reasoning</li>
                            </ul>
                        </div>
                        <div class="frontier">
                            <h3>Temporal Modeling</h3>
                            <ul>
                                <li>Video generation</li>
                                <li>Temporal consistency</li>
                                <li>Action understanding</li>
                            </ul>
                        </div>
                        <div class="frontier">
                            <h3>Interactive AI</h3>
                            <ul>
                                <li>Embodied agents</li>
                                <li>Real-time interaction</li>
                                <li>Multimodal dialogue</li>
                            </ul>
                        </div>
                    </div>
                </section>
            </section>

            <!-- Interview Questions -->
            <section>
                <section data-background-color="#059669">
                    <h1>‚ùì Interview Deep Dives</h1>
                    <p>Technical questions you might encounter</p>
                </section>

                <section>
                    <h2>üåä Diffusion Model Questions</h2>
                    <div class="interview-qa">
                        <h3 class="question">Q: "Explain the training process of diffusion models"</h3>
                        <div class="answer">
                            <ol>
                                <li><strong>Sample timestep t</strong> uniformly from [1, T]</li>
                                <li><strong>Sample noise Œµ</strong> from N(0, I)</li>
                                <li><strong>Create noisy image</strong>: x_t = ‚àöŒ±_t ¬∑ x_0 + ‚àö(1-Œ±_t) ¬∑ Œµ</li>
                                <li><strong>Predict noise</strong>: Œµ_Œ∏(x_t, t)</li>
                                <li><strong>Compute loss</strong>: ||Œµ - Œµ_Œ∏(x_t, t)||¬≤</li>
                            </ol>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üé≠ Multimodal Questions</h2>
                    <div class="interview-qa">
                        <h3 class="question">Q: "How would you handle modality imbalance in training?"</h3>
                        <div class="answer">
                            <ul>
                                <li><strong>Data balancing</strong>: Ensure equal representation</li>
                                <li><strong>Loss weighting</strong>: Adjust loss contributions</li>
                                <li><strong>Curriculum learning</strong>: Gradual complexity increase</li>
                                <li><strong>Modality dropout</strong>: Random modality masking</li>
                                <li><strong>Specialized architectures</strong>: Modality-specific encoders</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üöÄ System Design Question</h2>
                    <div class="interview-qa">
                        <h3 class="question">Q: "Design a real-time multimodal AI system for video understanding"</h3>
                        <div class="answer">
                            <h4>Architecture Components:</h4>
                            <ul>
                                <li><strong>Video encoder</strong>: Efficient temporal modeling</li>
                                <li><strong>Audio encoder</strong>: Speech and sound processing</li>
                                <li><strong>Text encoder</strong>: Caption and metadata processing</li>
                                <li><strong>Fusion layer</strong>: Multi-head cross-attention</li>
                                <li><strong>Streaming pipeline</strong>: Real-time processing</li>
                            </ul>
                            <h4>Optimization Strategies:</h4>
                            <ul>
                                <li><strong>Model quantization</strong>: INT8 inference</li>
                                <li><strong>Temporal chunking</strong>: Process video segments</li>
                                <li><strong>Caching</strong>: Store computed features</li>
                                <li><strong>Load balancing</strong>: Distribute across GPUs</li>
                            </ul>
                        </div>
                    </div>
                </section>
            </section>

            <!-- Next Steps -->
            <section data-background-gradient="linear-gradient(45deg, #059669, #7c3aed)">
                <h1>üéØ Mastery Checklist</h1>
                <div class="mastery-checklist">
                    <h2>Diffusion Models</h2>
                    <ul>
                        <li>‚úÖ Understand forward/reverse processes</li>
                        <li>‚úÖ Know sampling trade-offs</li>
                        <li>‚úÖ Familiar with conditioning techniques</li>
                        <li>‚úÖ Can explain training objective</li>
                    </ul>
                    
                    <h2>Multimodal AI</h2>
                    <ul>
                        <li>‚úÖ Understand fusion strategies</li>
                        <li>‚úÖ Know major VL models</li>
                        <li>‚úÖ Familiar with evaluation metrics</li>
                        <li>‚úÖ Can design multimodal systems</li>
                    </ul>
                </div>
                <p><a href="index.html" style="color: #ff6b35;">‚Üê Back to Main Slideshow</a></p>
            </section>
        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/reveal.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/notes/notes.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/markdown/markdown.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/highlight/highlight.min.js"></script>
    <script src="script.js"></script>
</body>
</html>
