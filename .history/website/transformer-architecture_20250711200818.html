<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Transformer Architecture - PrepMe</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/github-dark.min.css">
  <style>
    :root {
      --primary-gradient: linear-gradient(135deg, #66dfea 0%, #c3c91d 100%);
      --secondary-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      --card-bg: rgba(0,0,0,0.15);
      --card-hover: rgba(0,0,0,0.25);
      --accent-color: #4CAF50;
      --text-primary: #fff;
      --text-secondary: #e0e0e0;
      --shadow: 0 4px 16px rgba(0,0,0,0.1);
      --border-radius: 1rem;
    }

    * {
      box-sizing: border-box;
    }

    body {
      font-family: 'Segoe UI', Arial, sans-serif;
      background: var(--primary-gradient);
      color: var(--text-primary);
      margin: 0;
      min-height: 100vh;
      line-height: 1.6;
    }

    /* Header & Navigation */
    header {
      padding: 2rem 1rem 1rem 1rem;
      text-align: center;
      position: relative;
    }

    .header-content {
      max-width: 1200px;
      margin: 0 auto;
    }

    h1 {
      font-size: 3rem;
      margin-bottom: 0.5rem;
      background: linear-gradient(45deg, #fff, #4CAF50);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      animation: glow 2s ease-in-out infinite alternate;
    }

    @keyframes glow {
      from { filter: drop-shadow(0 0 5px rgba(76, 175, 80, 0.5)); }
      to { filter: drop-shadow(0 0 20px rgba(76, 175, 80, 0.8)); }
    }

    .subtitle {
      font-size: 1.3rem;
      color: var(--text-secondary);
      margin-bottom: 2rem;
    }

    /* Navigation */
    nav {
      margin: 2rem 0;
      display: flex;
      gap: 1.5rem;
      flex-wrap: wrap;
      justify-content: center;
    }

    .nav-card {
      background: var(--card-bg);
      color: var(--text-primary);
      padding: 1.5rem 2rem;
      border-radius: var(--border-radius);
      text-decoration: none;
      font-weight: 600;
      transition: all 0.3s ease;
      box-shadow: var(--shadow);
      border: 1px solid rgba(255,255,255,0.1);
      position: relative;
      overflow: hidden;
      min-width: 200px;
      text-align: center;
    }

    .nav-card:hover {
      background: var(--card-hover);
      transform: translateY(-5px);
      box-shadow: 0 8px 25px rgba(0,0,0,0.2);
      border-color: var(--accent-color);
    }

    /* Content */
    .content {
      max-width: 1200px;
      margin: 0 auto;
      padding: 2rem;
    }

    .section {
      background: var(--card-bg);
      margin: 2rem 0;
      padding: 2rem;
      border-radius: var(--border-radius);
      box-shadow: var(--shadow);
      border: 1px solid rgba(255,255,255,0.1);
    }

    h2 {
      color: var(--accent-color);
      font-size: 2rem;
      margin-bottom: 1rem;
      border-bottom: 2px solid var(--accent-color);
      padding-bottom: 0.5rem;
    }

    h3 {
      color: #66dfea;
      font-size: 1.5rem;
      margin: 1.5rem 0 1rem 0;
    }

    h4 {
      color: #c3c91d;
      font-size: 1.2rem;
      margin: 1rem 0 0.5rem 0;
    }

    p {
      margin: 1rem 0;
      line-height: 1.8;
    }

    ul, ol {
      margin: 1rem 0;
      padding-left: 2rem;
    }

    li {
      margin: 0.5rem 0;
    }

    strong {
      color: var(--accent-color);
    }

    .math-display {
      background: rgba(0,0,0,0.2);
      padding: 1.5rem;
      border-radius: 0.5rem;
      margin: 1rem 0;
      text-align: center;
      font-size: 1.2rem;
      border: 1px solid rgba(255,255,255,0.1);
    }

    .code-block {
      background: #1e1e1e;
      border-radius: 0.5rem;
      margin: 1rem 0;
      overflow-x: auto;
      border: 1px solid rgba(255,255,255,0.1);
    }

    .code-block pre {
      margin: 0;
      padding: 1rem;
    }

    .code-block code {
      font-family: 'Fira Code', 'Monaco', 'Consolas', monospace;
      font-size: 0.9rem;
      line-height: 1.4;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1rem 0;
      background: rgba(0,0,0,0.2);
      border-radius: 0.5rem;
      overflow: hidden;
    }

    th, td {
      padding: 1rem;
      text-align: left;
      border-bottom: 1px solid rgba(255,255,255,0.1);
    }

    th {
      background: rgba(76, 175, 80, 0.2);
      color: var(--accent-color);
      font-weight: bold;
    }

    .checklist {
      background: rgba(0,0,0,0.2);
      padding: 1.5rem;
      border-radius: 0.5rem;
      margin: 1rem 0;
    }

    .checklist-item {
      display: flex;
      align-items: center;
      margin: 0.5rem 0;
    }

    .checklist-item input[type="checkbox"] {
      margin-right: 1rem;
      transform: scale(1.2);
    }

    /* Footer */
    footer {
      margin-top: 4rem;
      padding: 2rem;
      text-align: center;
      background: rgba(0,0,0,0.1);
      border-top: 1px solid rgba(255,255,255,0.1);
    }

    .footer-content {
      max-width: 1200px;
      margin: 0 auto;
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
      gap: 2rem;
      text-align: left;
    }

    .footer-section h4 {
      color: var(--accent-color);
      margin-bottom: 1rem;
    }

    .footer-section a {
      color: var(--text-secondary);
      text-decoration: none;
      display: block;
      margin-bottom: 0.5rem;
      transition: color 0.3s ease;
    }

    .footer-section a:hover {
      color: var(--text-primary);
    }

    /* Responsive Design */
    @media (max-width: 768px) {
      h1 { font-size: 2rem; }
      .nav-card { min-width: 150px; padding: 1rem; }
      .content { padding: 1rem; }
      .section { padding: 1rem; }
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
</head>
<body>
  <header>
    <div class="header-content">
      <h1>Transformer Architecture Deep Dive</h1>
      <p class="subtitle">Master the revolutionary architecture that changed AI forever</p>
    </div>
  </header>

  <!-- Navigation -->
  <nav>
    <a href="index.html" class="nav-card">
      <h3>üè† Home</h3>
      <p>Back to main page</p>
    </a>
    <a href="large-language-models.html" class="nav-card">
      <h3>ü§ñ LLMs</h3>
      <p>Large Language Models</p>
    </a>
    <a href="diffusion-models.html" class="nav-card">
      <h3>üé® Diffusion</h3>
      <p>Diffusion Models</p>
    </a>
    <a href="multimodal-ai.html" class="nav-card">
      <h3>üñºÔ∏è Multimodal</h3>
      <p>Multimodal AI</p>
    </a>
  </nav>

  <div class="content">
    <!-- Learning Objectives -->
    <div class="section">
      <h2>üéØ Learning Objectives</h2>
      <p>By the end of this section, you'll be able to:</p>
      <ul>
        <li>Explain the transformer architecture from memory</li>
        <li>Understand why transformers revolutionized NLP and beyond</li>
        <li>Compare different transformer variants and their use cases</li>
        <li>Implement key components of a transformer</li>
      </ul>
    </div>

    <!-- Architecture Overview -->
    <div class="section">
      <h2>üèóÔ∏è Architecture Overview</h2>
      <h3>The Big Picture</h3>
      <div class="code-block">
        <pre><code>Input ‚Üí Embedding ‚Üí Positional Encoding ‚Üí Encoder/Decoder Layers ‚Üí Output</code></pre>
      </div>
      <p><strong>Key Innovation</strong>: Self-attention mechanism allows parallel processing and captures long-range dependencies better than RNNs.</p>
    </div>

    <!-- Self-Attention Mechanism -->
    <div class="section">
      <h2>üîç Self-Attention Mechanism</h2>
      
      <h3>Mathematical Foundation</h3>
      <div class="math-display">
        $$\text{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
      </div>

      <h4>Components Explained</h4>
      <ul>
        <li><strong>Q (Query)</strong>: "What am I looking for?" - represents the current position/token</li>
        <li><strong>K (Key)</strong>: "What information is available?" - all positions in the sequence</li>
        <li><strong>V (Value)</strong>: "What is the actual information?" - the content to be retrieved</li>
        <li><strong>‚àöd_k scaling</strong>: Prevents softmax saturation in high dimensions</li>
      </ul>

      <h3>Intuitive Understanding</h3>
      <p>Think of attention as a <strong>soft database lookup</strong>:</p>
      <ol>
        <li><strong>Query</strong>: Your search term</li>
        <li><strong>Keys</strong>: Database indices</li>
        <li><strong>Values</strong>: Actual data stored</li>
        <li><strong>Attention weights</strong>: How relevant each database entry is to your query</li>
      </ol>

      <h3>Step-by-Step Process</h3>
      <ol>
        <li><strong>Linear Transformations</strong>: Input embeddings ‚Üí Q, K, V matrices</li>
        <li><strong>Attention Scores</strong>: Compute QK^T (how much each position attends to others)</li>
        <li><strong>Scaling</strong>: Divide by ‚àöd_k to prevent gradient vanishing</li>
        <li><strong>Softmax</strong>: Convert scores to probabilities (attention weights)</li>
        <li><strong>Weighted Sum</strong>: Multiply attention weights by values</li>
      </ol>
    </div>

    <!-- Multi-Head Attention -->
    <div class="section">
      <h2>üéØ Multi-Head Attention</h2>
      
      <h3>Why Multiple Heads?</h3>
      <p>Single attention head might focus on one type of relationship. Multiple heads can capture:</p>
      <ul>
        <li><strong>Syntactic relationships</strong> (subject-verb agreement)</li>
        <li><strong>Semantic relationships</strong> (word meanings)</li>
        <li><strong>Positional relationships</strong> (nearby vs distant words)</li>
        <li><strong>Different abstraction levels</strong></li>
      </ul>

      <h3>Mathematical Formulation</h3>
      <div class="code-block">
        <pre><code>MultiHead(Q,K,V) = Concat(head‚ÇÅ, head‚ÇÇ, ..., head_h)W^O

where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</code></pre>
      </div>

      <h3>Implementation Details</h3>
      <ul>
        <li><strong>Typical setup</strong>: 8-16 heads for base models, 32+ for large models</li>
        <li><strong>Dimension per head</strong>: d_model / h (e.g., 768/12 = 64 for BERT-base)</li>
        <li><strong>Parameter sharing</strong>: Each head has its own W^Q, W^K, W^V matrices</li>
      </ul>
    </div>

    <!-- Positional Encoding -->
    <div class="section">
      <h2>üìç Positional Encoding</h2>
      
      <h3>The Problem</h3>
      <p>Transformers process all positions simultaneously ‚Üí <strong>no inherent sequence understanding</strong></p>

      <h3>Sinusoidal Encoding (Original Transformer)</h3>
      <div class="code-block">
        <pre><code>PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</code></pre>
      </div>

      <h4>Properties</h4>
      <ul>
        <li><strong>Deterministic</strong>: Same position always gets same encoding</li>
        <li><strong>Relative distances</strong>: PE(pos+k) can be expressed as linear combination of PE(pos)</li>
        <li><strong>Extrapolation</strong>: Can handle sequences longer than training</li>
      </ul>

      <h3>Learned Positional Embeddings</h3>
      <ul>
        <li><strong>Trainable parameters</strong> for each position</li>
        <li><strong>Better performance</strong> on fixed-length sequences</li>
        <li><strong>No extrapolation</strong> beyond training length</li>
        <li>Used in BERT, GPT</li>
      </ul>

      <h3>Advanced Positional Encodings</h3>
      <p><strong>Rotary Position Embedding (RoPE)</strong>:</p>
      <ul>
        <li>Encodes positions as rotations in complex space</li>
        <li>Better length extrapolation</li>
        <li>Used in LLaMA, GPT-NeoX</li>
      </ul>

      <p><strong>Alibi (Attention with Linear Biases)</strong>:</p>
      <ul>
        <li>Adds position-dependent bias to attention scores</li>
        <li>Very good extrapolation properties</li>
        <li>Used in some recent models</li>
      </ul>
    </div>

    <!-- Transformer Variants -->
    <div class="section">
      <h2>üèóÔ∏è Transformer Variants</h2>
      
      <h3>1. Encoder-Only (BERT-style)</h3>
      <div class="code-block">
        <pre><code>Input ‚Üí [CLS] token‚ÇÅ token‚ÇÇ ... [SEP] ‚Üí Bidirectional attention ‚Üí Outputs</code></pre>
      </div>

      <h4>Characteristics</h4>
      <ul>
        <li><strong>Bidirectional context</strong>: Can see entire sequence</li>
        <li><strong>Masked Language Modeling</strong>: Predict masked tokens</li>
        <li><strong>Use cases</strong>: Classification, NER, question answering</li>
      </ul>

      <h4>When to Use</h4>
      <ul>
        <li>Need understanding of entire context</li>
        <li>Classification tasks</li>
        <li>Tasks where you have complete input upfront</li>
      </ul>

      <h3>2. Decoder-Only (GPT-style)</h3>
      <div class="code-block">
        <pre><code>Input ‚Üí token‚ÇÅ token‚ÇÇ ... ‚Üí Causal attention (triangular mask) ‚Üí Next token</code></pre>
      </div>

      <h4>Characteristics</h4>
      <ul>
        <li><strong>Causal masking</strong>: Can only see previous tokens</li>
        <li><strong>Autoregressive generation</strong>: Predict next token</li>
        <li><strong>Use cases</strong>: Text generation, completion, dialogue</li>
      </ul>

      <h4>When to Use</h4>
      <ul>
        <li>Text generation tasks</li>
        <li>Need streaming/online processing</li>
        <li>Want single model for multiple tasks</li>
      </ul>

      <h3>3. Encoder-Decoder (T5-style)</h3>
      <div class="code-block">
        <pre><code>Encoder: Input ‚Üí Bidirectional attention ‚Üí Context
Decoder: Output tokens ‚Üí Causal attention + Cross-attention to encoder ‚Üí Generation</code></pre>
      </div>

      <h4>Characteristics</h4>
      <ul>
        <li><strong>Separate encoding/decoding</strong>: Different objectives</li>
        <li><strong>Cross-attention</strong>: Decoder attends to encoder outputs</li>
        <li><strong>Use cases</strong>: Translation, summarization, seq2seq tasks</li>
      </ul>

      <h4>When to Use</h4>
      <ul>
        <li>Input and output are different modalities/languages</li>
        <li>Clear separation between understanding and generation phases</li>
        <li>Complex structured outputs</li>
      </ul>
    </div>

    <!-- Key Architectural Components -->
    <div class="section">
      <h2>üîß Key Architectural Components</h2>
      
      <h3>Layer Normalization</h3>
      <div class="math-display">
        $$\text{LayerNorm}(x) = \gamma * \frac{x - \mu}{\sigma} + \beta$$
      </div>

      <h4>Pre-norm vs Post-norm</h4>
      <ul>
        <li><strong>Pre-norm</strong>: LayerNorm ‚Üí Attention/FFN ‚Üí Residual (more stable training)</li>
        <li><strong>Post-norm</strong>: Attention/FFN ‚Üí LayerNorm ‚Üí Residual (original design)</li>
      </ul>

      <p><strong>RMSNorm</strong> (used in LLaMA):</p>
      <ul>
        <li>Removes mean centering: RMS(x) = ‚àö(Œ£x¬≤/n)</li>
        <li>Faster computation, similar performance</li>
      </ul>

      <h3>Feed-Forward Networks</h3>
      <div class="math-display">
        $$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$
      </div>

      <h4>Modern Variants</h4>
      <ul>
        <li><strong>SwiGLU</strong>: Swish activation + Gated Linear Unit</li>
        <li><strong>GeGLU</strong>: GELU activation + GLU</li>
        <li><strong>Typically 4x wider</strong> than attention dimension</li>
      </ul>

      <h3>Residual Connections</h3>
      <div class="code-block">
        <pre><code>output = LayerNorm(x + SubLayer(x))</code></pre>
      </div>

      <h4>Benefits</h4>
      <ul>
        <li><strong>Gradient flow</strong>: Enables training very deep networks</li>
        <li><strong>Identity mapping</strong>: Model can learn to ignore layers if needed</li>
        <li><strong>Stability</strong>: Reduces internal covariate shift</li>
      </ul>
    </div>

    <!-- Scaling Laws & Model Sizes -->
    <div class="section">
      <h2>üìä Scaling Laws & Model Sizes</h2>
      
      <h3>Parameter Scaling</h3>
      <table>
        <thead>
          <tr>
            <th>Model</th>
            <th>Parameters</th>
            <th>Layers</th>
            <th>Hidden Size</th>
            <th>Attention Heads</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>BERT-base</td>
            <td>110M</td>
            <td>12</td>
            <td>768</td>
            <td>12</td>
          </tr>
          <tr>
            <td>BERT-large</td>
            <td>340M</td>
            <td>24</td>
            <td>1024</td>
            <td>16</td>
          </tr>
          <tr>
            <td>GPT-3</td>
            <td>175B</td>
            <td>96</td>
            <td>12288</td>
            <td>96</td>
          </tr>
          <tr>
            <td>GPT-4</td>
            <td>~1.7T</td>
            <td>~120</td>
            <td>~18432</td>
            <td>~128</td>
          </tr>
        </tbody>
      </table>

      <h3>Compute Scaling</h3>
      <ul>
        <li><strong>Training FLOPs</strong>: ‚âà 6 √ó Parameters √ó Tokens</li>
        <li><strong>Inference FLOPs</strong>: ‚âà 2 √ó Parameters √ó Generated tokens</li>
      </ul>

      <h3>Memory Requirements</h3>
      <ul>
        <li><strong>Training</strong>: 4 √ó Parameters (weights + gradients + optimizer states)</li>
        <li><strong>Inference</strong>: 2 √ó Parameters (weights + activations)</li>
      </ul>
    </div>

    <!-- Interview Questions & Answers -->
    <div class="section">
      <h2>üéØ Interview Questions & Answers</h2>
      
      <h3>Q1: "Why did transformers replace RNNs for many NLP tasks?"</h3>
      <p><strong>Answer Framework</strong>:</p>
      <ol>
        <li><strong>Parallelization</strong>: RNNs process sequentially, transformers process all positions simultaneously</li>
        <li><strong>Long-range dependencies</strong>: Attention provides direct connections between any two positions</li>
        <li><strong>Training efficiency</strong>: Parallel processing enables faster training on modern hardware</li>
        <li><strong>Scalability</strong>: Architecture scales better to larger models and datasets</li>
        <li><strong>Transfer learning</strong>: Pre-trained transformers transfer better across tasks</li>
      </ol>

      <h3>Q2: "Explain the intuition behind attention weights"</h3>
      <p><strong>Answer Framework</strong>:</p>
      <ol>
        <li><strong>Relevance scoring</strong>: How relevant is each position to the current query</li>
        <li><strong>Soft selection</strong>: Instead of hard selection, we take weighted average</li>
        <li><strong>Context aggregation</strong>: Combines information from multiple relevant positions</li>
        <li><strong>Learned associations</strong>: Model learns what to attend to during training</li>
      </ol>

      <h3>Q3: "What are the computational bottlenecks in transformers?"</h3>
      <p><strong>Answer Framework</strong>:</p>
      <ol>
        <li><strong>Attention complexity</strong>: O(n¬≤) memory and computation with sequence length</li>
        <li><strong>Memory bandwidth</strong>: Moving large matrices between memory and compute</li>
        <li><strong>Sequence length scaling</strong>: Quadratic scaling limits very long sequences</li>
        <li><strong>Solutions</strong>: Sparse attention, linear attention, Flash Attention, chunking</li>
      </ol>

      <h3>Q4: "How would you modify a transformer for very long sequences?"</h3>
      <p><strong>Answer Framework</strong>:</p>
      <ol>
        <li><strong>Sparse attention patterns</strong>: Only attend to subset of positions</li>
        <li><strong>Sliding window attention</strong>: Local attention with some global connections</li>
        <li><strong>Hierarchical attention</strong>: Attend at multiple resolutions</li>
        <li><strong>Memory mechanisms</strong>: External memory for very long contexts</li>
        <li><strong>Retrieval augmentation</strong>: Retrieve relevant segments instead of processing all</li>
      </ol>
    </div>

    <!-- Implementation Insights -->
    <div class="section">
      <h2>üî® Implementation Insights</h2>
      
      <h3>Efficient Attention Computation</h3>
      <div class="code-block">
        <pre><code class="language-python"># Flash Attention insight: recompute attention on-the-fly to save memory
def flash_attention(Q, K, V, block_size):
    # Process in blocks to fit in SRAM
    # Recompute attention weights instead of storing them
    # Achieves O(n¬≤) compute but O(n) memory</code></pre>
      </div>

      <h3>Multi-Head Attention Tricks</h3>
      <div class="code-block">
        <pre><code class="language-python"># Parallel computation of all heads
def multi_head_attention(x, W_qkv, W_o, num_heads):
    # Single matrix multiplication for all Q, K, V
    qkv = x @ W_qkv  # [batch, seq, 3 * hidden]
    
    # Reshape and transpose for parallel head computation
    q, k, v = qkv.chunk(3, dim=-1)
    q = q.view(batch, seq, num_heads, head_dim).transpose(1, 2)
    # ... similar for k, v</code></pre>
      </div>

      <h3>Position Encoding Implementation</h3>
      <div class="code-block">
        <pre><code class="language-python">def sinusoidal_pos_encoding(seq_len, d_model):
    position = torch.arange(seq_len).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2) * 
                        -(math.log(10000.0) / d_model))
    
    pe = torch.zeros(seq_len, d_model)
    pe[:, 0::2] = torch.sin(position * div_term)  # even indices
    pe[:, 1::2] = torch.cos(position * div_term)  # odd indices
    return pe</code></pre>
      </div>
    </div>

    <!-- Advanced Topics -->
    <div class="section">
      <h2>üöÄ Advanced Topics for Senior Role</h2>
      
      <h3>Architectural Innovations</h3>
      <ol>
        <li><strong>Mixture of Experts (MoE)</strong>: Sparse activation of expert networks</li>
        <li><strong>Switch Transformer</strong>: Simple and efficient MoE routing</li>
        <li><strong>GLaM</strong>: Sparsely activated language model</li>
        <li><strong>PaLM</strong>: Pathways Language Model with improved training</li>
      </ol>

      <h3>Optimization Techniques</h3>
      <ol>
        <li><strong>Gradient checkpointing</strong>: Trade compute for memory</li>
        <li><strong>Mixed precision</strong>: FP16 forward, FP32 gradients</li>
        <li><strong>ZeRO</strong>: Optimizer state sharding across devices</li>
        <li><strong>3D parallelism</strong>: Data + model + pipeline parallelism</li>
      </ol>

      <h3>Research Directions</h3>
      <ol>
        <li><strong>Linear attention</strong>: Reducing quadratic complexity</li>
        <li><strong>Retrieval augmentation</strong>: External memory mechanisms</li>
        <li><strong>Multimodal transformers</strong>: Vision, audio, text integration</li>
        <li><strong>Efficient architectures</strong>: MobileBERT, DistilBERT, TinyBERT</li>
      </ol>
    </div>

    <!-- Study Checklist -->
    <div class="section">
      <h2>üìù Study Checklist</h2>
      <div class="checklist">
        <div class="checklist-item">
          <input type="checkbox" id="check1">
          <label for="check1">Can draw transformer architecture from memory</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="check2">
          <label for="check2">Understand attention mechanism mathematically and intuitively</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="check3">
          <label for="check3">Know when to use encoder-only vs decoder-only vs encoder-decoder</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="check4">
          <label for="check4">Understand positional encoding options and trade-offs</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="check5">
          <label for="check5">Can discuss scaling laws and computational requirements</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="check6">
          <label for="check6">Know recent architectural innovations and optimizations</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="check7">
          <label for="check7">Can implement basic transformer components</label>
        </div>
        <div class="checklist-item">
          <input type="checkbox" id="check8">
          <label for="check8">Understand production deployment considerations</label>
        </div>
      </div>
    </div>
  </div>

  <footer>
    <div class="footer-content">
      <div class="footer-section">
        <h4>üìö Core Concepts</h4>
        <a href="transformer-architecture.html">Transformer Architecture</a>
        <a href="large-language-models.html">Large Language Models</a>
        <a href="diffusion-models.html">Diffusion Models</a>
        <a href="multimodal-ai.html">Multimodal AI</a>
      </div>
      <div class="footer-section">
        <h4>üî¨ Research & Implementation</h4>
        <a href="research.html">Research Methodology</a>
        <a href="code.html">Code Examples</a>
        <a href="resources.html">Resources</a>
      </div>
      <div class="footer-section">
        <h4>‚òÅÔ∏è AWS Production</h4>
        <a href="slides.html">AWS Services</a>
        <a href="resources.html">Production ML</a>
      </div>
      <div class="footer-section">
        <h4>üìû Support</h4>
        <a href="index.html">Home</a>
        <a href="resources.html">Resources</a>
      </div>
    </div>
    <div style="margin-top: 2rem; padding-top: 1rem; border-top: 1px solid rgba(255,255,255,0.1);">
      &copy; 2024 PrepMe | Generative AI & Applied Science Interview Prep
    </div>
  </footer>

  <script>
    // Initialize
    window.onload = function() {
      // Highlight code
      document.querySelectorAll('pre code').forEach(el => hljs.highlightElement(el));
      
      // Save checklist progress
      document.querySelectorAll('.checklist-item input').forEach(checkbox => {
        checkbox.addEventListener('change', function() {
          const checkboxes = document.querySelectorAll('.checklist-item input:checked');
          const total = document.querySelectorAll('.checklist-item input').length;
          const progress = Math.round((checkboxes.length / total) * 100);
          
          // Save to localStorage
          localStorage.setItem('transformerProgress', progress);
          
          // Update progress display if exists
          const progressElement = document.getElementById('progressPercent');
          if (progressElement) {
            progressElement.textContent = progress + '%';
          }
        });
        
        // Load saved state
        const savedProgress = localStorage.getItem('transformerProgress') || 0;
        const checkboxes = document.querySelectorAll('.checklist-item input');
        const checkedCount = Math.floor((savedProgress / 100) * checkboxes.length);
        
        for (let i = 0; i < checkedCount; i++) {
          checkboxes[i].checked = true;
        }
      });
    };
  </script>
</body>
</html>
