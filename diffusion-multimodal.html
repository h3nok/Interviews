<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Diffusion Models & Multimodal AI - AWS Interview Prep</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/reveal.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/theme/black.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/monokai.min.css">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide -->
            <section data-background-gradient="linear-gradient(45deg, #059669, #1e3a8a)">
                <h1>üé® Diffusion Models & Multimodal AI</h1>
                <p>Advanced generative models for the modern AI researcher</p>
                <div class="navigation-hint">
                    <p><a href="index.html" style="color: #ff6b35;">‚Üê Back to Main</a> | Press <kbd>‚Üí</kbd> to continue</p>
                </div>
            </section>

            <!-- Diffusion Models Section -->
            <section>
                <section data-background-color="#059669">
                    <h1>üåä Diffusion Models</h1>
                    <p>Probabilistic generative models that learn to reverse noise</p>
                </section>

                <section>
                    <h2>üß† Core Intuition</h2>
                    <div class="diffusion-process">
                        <h3>Forward Process (Noise Addition)</h3>
                        <div class="process-step">
                            <span>Clean Image</span> ‚Üí <span>+ Noise</span> ‚Üí <span>+ More Noise</span> ‚Üí <span>Pure Noise</span>
                        </div>
                        
                        <h3>Reverse Process (Denoising)</h3>
                        <div class="process-step reverse">
                            <span>Pure Noise</span> ‚Üí <span>- Noise</span> ‚Üí <span>- More Noise</span> ‚Üí <span>Clean Image</span>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üìê Mathematical Foundation Deep Dive</h2>
                    <div class="math-section">
                        <h3>Forward Process (Markov Chain)</h3>
                        <div class="formula">
                            <code>q(x<sub>t</sub>|x<sub>t-1</sub>) = N(x<sub>t</sub>; ‚àö(1-Œ≤<sub>t</sub>)x<sub>t-1</sub>, Œ≤<sub>t</sub>I)</code>
                        </div>
                        <div class="formula-small">
                            <code>q(x<sub>1:T</sub>|x<sub>0</sub>) = ‚àè<sub>t=1</sub><sup>T</sup> q(x<sub>t</sub>|x<sub>t-1</sub>)</code>
                        </div>
                        <p><strong>Key insight:</strong> Gradual noise addition preserves structure in early steps</p>
                        
                        <h3>Closed-Form Forward Process</h3>
                        <div class="formula">
                            <code>q(x<sub>t</sub>|x<sub>0</sub>) = N(x<sub>t</sub>; ‚àö·æ±<sub>t</sub>x<sub>0</sub>, (1-·æ±<sub>t</sub>)I)</code>
                        </div>
                        <div class="formula-small">
                            <code>where ·æ±<sub>t</sub> = ‚àè<sub>s=1</sub><sup>t</sup> (1-Œ≤<sub>s</sub>)</code>
                        </div>
                        <p><strong>Reparameterization:</strong> x<sub>t</sub> = ‚àö·æ±<sub>t</sub>x<sub>0</sub> + ‚àö(1-·æ±<sub>t</sub>)Œµ</p>
                        
                        <h3>Reverse Process (Learned Denoising)</h3>
                        <div class="formula">
                            <code>p<sub>Œ∏</sub>(x<sub>t-1</sub>|x<sub>t</sub>) = N(x<sub>t-1</sub>; Œº<sub>Œ∏</sub>(x<sub>t</sub>,t), Œ£<sub>Œ∏</sub>(x<sub>t</sub>,t))</code>
                        </div>
                        <div class="formula-small">
                            <code>p<sub>Œ∏</sub>(x<sub>0:T</sub>) = p(x<sub>T</sub>) ‚àè<sub>t=1</sub><sup>T</sup> p<sub>Œ∏</sub>(x<sub>t-1</sub>|x<sub>t</sub>)</code>
                        </div>
                        
                        <h3>ELBO and Training Objective</h3>
                        <div class="formula">
                            <code>L<sub>VLB</sub> = E<sub>q</sub>[-log p<sub>Œ∏</sub>(x<sub>0</sub>|x<sub>1</sub>)] + ‚àë<sub>t=2</sub><sup>T</sup> E<sub>q</sub>[KL(q(x<sub>t-1</sub>|x<sub>t</sub>,x<sub>0</sub>)||p<sub>Œ∏</sub>(x<sub>t-1</sub>|x<sub>t</sub>))]</code>
                        </div>
                        <div class="formula">
                            <code>L<sub>simple</sub> = E<sub>t,x<sub>0</sub>,Œµ</sub>[||Œµ - Œµ<sub>Œ∏</sub>(x<sub>t</sub>, t)||¬≤]</code>
                        </div>
                        <p><strong>Surprising result:</strong> Simple L2 loss works better than complex ELBO!</p>
                        
                        <h3>Variance Schedules</h3>
                        <div class="technical-details">
                            <h4>Common Schedules:</h4>
                            <ul>
                                <li><strong>Linear:</strong> Œ≤<sub>t</sub> = Œ≤<sub>1</sub> + (Œ≤<sub>T</sub> - Œ≤<sub>1</sub>)(t-1)/(T-1)</li>
                                <li><strong>Cosine:</strong> ·æ±<sub>t</sub> = cos¬≤((t/T + s)/(1 + s) ¬∑ œÄ/2)</li>
                                <li><strong>Sigmoid:</strong> Smooth transitions, better for high-res images</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üèóÔ∏è Architecture Deep Dive: U-Net for Diffusion</h2>
                    <div class="unet-components">
                        <div class="component">
                            <h3>Encoder (Downsampling)</h3>
                            <ul>
                                <li><strong>ResNet blocks:</strong> Skip connections for gradient flow</li>
                                <li><strong>Self-attention:</strong> At 16√ó16 and 8√ó8 resolutions</li>
                                <li><strong>Group normalization:</strong> Better than batch norm for generation</li>
                                <li><strong>Downsampling:</strong> 2√ó2 conv with stride 2</li>
                                <li><strong>Time embedding:</strong> Sinusoidal positional encoding</li>
                            </ul>
                        </div>
                        <div class="component">
                            <h3>Bottleneck</h3>
                            <ul>
                                <li><strong>Lowest resolution:</strong> Typically 8√ó8 for 512√ó512 images</li>
                                <li><strong>Maximum channels:</strong> Often 1024 or 1280</li>
                                <li><strong>Global attention:</strong> Full self-attention at this scale</li>
                                <li><strong>Cross-attention:</strong> For text conditioning (if applicable)</li>
                                <li><strong>Heavy computation:</strong> Most parameters concentrated here</li>
                            </ul>
                        </div>
                        <div class="component">
                            <h3>Decoder (Upsampling)</h3>
                            <ul>
                                <li><strong>Skip connections:</strong> Concatenate with encoder features</li>
                                <li><strong>Upsampling layers:</strong> Nearest neighbor + conv or transpose conv</li>
                                <li><strong>Feature combination:</strong> ResNet blocks process concatenated features</li>
                                <li><strong>Progressive refinement:</strong> Coarse-to-fine generation</li>
                                <li><strong>Output layer:</strong> Final conv to RGB channels</li>
                            </ul>
                        </div>
                    </div>
                    
                    <div class="technical-details">
                        <h4>Key Architectural Insights:</h4>
                        <ul>
                            <li><strong>Skip connections crucial:</strong> Enable high-frequency detail preservation</li>
                            <li><strong>Attention at multiple scales:</strong> Captures both local and global structure</li>
                            <li><strong>Time conditioning:</strong> Allows network to adapt denoising to noise level</li>
                            <li><strong>Group normalization:</strong> More stable than batch norm for varying batch sizes</li>
                            <li><strong>Progressive architecture:</strong> Mirrors natural coarse-to-fine generation process</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>üéØ Key Innovations & Evolution</h2>
                    <div class="innovations-grid">
                        <div class="innovation">
                            <h3>DDPM (2020)</h3>
                            <p>Denoising Diffusion Probabilistic Models</p>
                            <span class="innovation-detail">Fixed noise schedule, U-Net architecture, established foundation</span>
                            <div class="technical-details">
                                <ul>
                                    <li><strong>T = 1000 steps:</strong> Slow but high quality</li>
                                    <li><strong>Variance schedule:</strong> Linear Œ≤ schedule</li>
                                    <li><strong>Key insight:</strong> Simple L2 loss outperforms complex ELBO</li>
                                </ul>
                            </div>
                        </div>
                        <div class="innovation">
                            <h3>DDIM (2021)</h3>
                            <p>Denoising Diffusion Implicit Models</p>
                            <span class="innovation-detail">Deterministic sampling, fewer steps, interpolatable</span>
                            <div class="technical-details">
                                <ul>
                                    <li><strong>50-200 steps:</strong> 5-20x faster than DDPM</li>
                                    <li><strong>Deterministic:</strong> Same input ‚Üí same output</li>
                                    <li><strong>Œ∑ parameter:</strong> Controls stochasticity (0 = deterministic)</li>
                                </ul>
                            </div>
                        </div>
                        <div class="innovation">
                            <h3>Classifier-Free Guidance (2022)</h3>
                            <p>Conditional generation without classifier</p>
                            <span class="innovation-detail">Better quality, easier training, widely adopted</span>
                            <div class="technical-details">
                                <ul>
                                    <li><strong>Guidance scale w:</strong> Controls conditioning strength</li>
                                    <li><strong>Formula:</strong> ŒµÃÉ = Œµ<sub>Œ∏</sub>(x<sub>t</sub>) + w¬∑(Œµ<sub>Œ∏</sub>(x<sub>t</sub>,c) - Œµ<sub>Œ∏</sub>(x<sub>t</sub>))</li>
                                    <li><strong>Null conditioning:</strong> Train with empty prompts ~10% of time</li>
                                </ul>
                            </div>
                        </div>
                        <div class="innovation">
                            <h3>Latent Diffusion (Stable Diffusion)</h3>
                            <p>Diffusion in latent space via VAE</p>
                            <span class="innovation-detail">Computational efficiency, high resolution, democratized AI</span>
                            <div class="technical-details">
                                <ul>
                                    <li><strong>VAE encoder:</strong> 512√ó512 ‚Üí 64√ó64√ó4</li>
                                    <li><strong>8x compression:</strong> 64x fewer pixels to process</li>
                                    <li><strong>Cross-attention:</strong> Text conditioning via CLIP embeddings</li>
                                </ul>
                            </div>
                        </div>
                        <div class="innovation">
                            <h3>DPM-Solver++ (2022)</h3>
                            <p>Fast high-order solvers for diffusion ODEs</p>
                            <span class="innovation-detail">10-20 steps, maintains quality, practical deployment</span>
                            <div class="technical-details">
                                <ul>
                                    <li><strong>High-order solver:</strong> Better approximation of reverse SDE</li>
                                    <li><strong>Multistep methods:</strong> Use history for better predictions</li>
                                    <li><strong>Practical impact:</strong> Enables real-time applications</li>
                                </ul>
                            </div>
                        </div>
                        <div class="innovation">
                            <h3>CONSISTENCY Models (2023)</h3>
                            <p>Single-step generation with quality guarantees</p>
                            <span class="innovation-detail">1-step generation, consistency training, breakthrough speed</span>
                            <div class="technical-details">
                                <ul>
                                    <li><strong>Consistency function:</strong> Maps any point on trajectory to x<sub>0</sub></li>
                                    <li><strong>Self-consistency:</strong> f(x<sub>t</sub>, t) = f(x<sub>t'</sub>, t') for same trajectory</li>
                                    <li><strong>Progressive training:</strong> Gradually increase number of discretization steps</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üîß Sampling Methods & Speed-Quality Trade-offs</h2>
                    <div class="sampling-methods">
                        <div class="method">
                            <h3>DDPM Sampling</h3>
                            <ul>
                                <li><strong>1000 steps:</strong> Original formulation, highest quality</li>
                                <li><strong>Stochastic process:</strong> Langevin dynamics</li>
                                <li><strong>Generation time:</strong> ~20 seconds on modern GPU</li>
                                <li><strong>Quality:</strong> Gold standard for evaluation</li>
                                <li><strong>Use case:</strong> Research, quality benchmarks</li>
                            </ul>
                            <div class="formula-small">
                                <code>x<sub>t-1</sub> = Œº<sub>Œ∏</sub>(x<sub>t</sub>, t) + œÉ<sub>t</sub>z, z ~ N(0,I)</code>
                            </div>
                        </div>
                        <div class="method">
                            <h3>DDIM Sampling</h3>
                            <ul>
                                <li><strong>50-200 steps:</strong> Configurable speed-quality trade-off</li>
                                <li><strong>Deterministic process:</strong> ODE solver approach</li>
                                <li><strong>Generation time:</strong> ~2-5 seconds</li>
                                <li><strong>Interpolation:</strong> Smooth latent space interpolation</li>
                                <li><strong>Use case:</strong> Production, interactive applications</li>
                            </ul>
                            <div class="formula-small">
                                <code>x<sub>t-1</sub> = ‚àöŒ±<sub>t-1</sub> xÃÇ<sub>0</sub> + ‚àö(1-Œ±<sub>t-1</sub>-œÉ<sub>t</sub>¬≤) Œµ<sub>Œ∏</sub> + œÉ<sub>t</sub>z</code>
                            </div>
                        </div>
                        <div class="method">
                            <h3>DPM-Solver++</h3>
                            <ul>
                                <li><strong>10-25 steps:</strong> Extreme speed optimization</li>
                                <li><strong>High-order method:</strong> Uses derivative information</li>
                                <li><strong>Generation time:</strong> ~1-2 seconds</li>
                                <li><strong>Quality preservation:</strong> Minimal degradation</li>
                                <li><strong>Use case:</strong> Real-time applications, mobile deployment</li>
                            </ul>
                            <div class="formula-small">
                                <code>Multistep predictor-corrector with noise schedule optimization</code>
                            </div>
                        </div>
                    </div>
                    
                    <div class="technical-details">
                        <h4>Performance Comparison (512√ó512 image on A100):</h4>
                        <table class="comparison-table">
                            <tr>
                                <th>Method</th>
                                <th>Steps</th>
                                <th>Time (s)</th>
                                <th>FID Score</th>
                                <th>Best Use Case</th>
                            </tr>
                            <tr>
                                <td>DDPM</td>
                                <td>1000</td>
                                <td>~20</td>
                                <td>3.17</td>
                                <td>Research, benchmarks</td>
                            </tr>
                            <tr>
                                <td>DDIM</td>
                                <td>50</td>
                                <td>~2</td>
                                <td>4.67</td>
                                <td>Production apps</td>
                            </tr>
                            <tr>
                                <td>DPM-Solver++</td>
                                <td>15</td>
                                <td>~1</td>
                                <td>4.80</td>
                                <td>Real-time systems</td>
                            </tr>
                            <tr>
                                <td>Consistency</td>
                                <td>1</td>
                                <td>~0.1</td>
                                <td>7.20</td>
                                <td>Interactive apps</td>
                            </tr>
                        </table>
                    </div>
                </section>
            </section>

            <!-- Multimodal AI Section -->
            <section>
                <section data-background-color="#7c3aed">
                    <h1>üé≠ Multimodal AI</h1>
                    <p>Bridging vision, language, and beyond</p>
                </section>

                <section>
                    <h2>üß© Modality Fusion Strategies Deep Dive</h2>
                    <div class="fusion-strategies">
                        <div class="strategy">
                            <h3>Early Fusion</h3>
                            <p>Combine features at input level</p>
                            <div class="pros-cons">
                                <div class="pros">‚úÖ Rich cross-modal interaction</div>
                                <div class="cons">‚ùå Modality mismatch, synchronization issues</div>
                            </div>
                            <div class="technical-details">
                                <h4>Implementation:</h4>
                                <ul>
                                    <li><strong>Concatenation:</strong> Simple feature stacking</li>
                                    <li><strong>Element-wise ops:</strong> Addition, multiplication</li>
                                    <li><strong>Shared encoders:</strong> Joint representation learning</li>
                                </ul>
                            </div>
                        </div>
                        <div class="strategy">
                            <h3>Late Fusion</h3>
                            <p>Process separately, combine at decision level</p>
                            <div class="pros-cons">
                                <div class="pros">‚úÖ Modality-specific processing, robust to missing modalities</div>
                                <div class="cons">‚ùå Limited cross-modal learning, potential information loss</div>
                            </div>
                            <div class="technical-details">
                                <h4>Implementation:</h4>
                                <ul>
                                    <li><strong>Voting/averaging:</strong> Simple decision combination</li>
                                    <li><strong>Attention weighting:</strong> Learned importance scores</li>
                                    <li><strong>MLP fusion:</strong> Learnable combination function</li>
                                </ul>
                            </div>
                        </div>
                        <div class="strategy">
                            <h3>Hybrid/Intermediate Fusion</h3>
                            <p>Multiple fusion points throughout network</p>
                            <div class="pros-cons">
                                <div class="pros">‚úÖ Best of both worlds, hierarchical understanding</div>
                                <div class="cons">‚ùå Increased complexity, more hyperparameters</div>
                            </div>
                            <div class="technical-details">
                                <h4>Modern Approaches:</h4>
                                <ul>
                                    <li><strong>Cross-attention:</strong> Query one modality, attend to another</li>
                                    <li><strong>Perceiver Resampler:</strong> Fixed-size bottleneck for variable inputs</li>
                                    <li><strong>Transformer blocks:</strong> Interleaved self and cross-attention</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    
                    <div class="technical-details">
                        <h4>Fusion Strategy Selection Guidelines:</h4>
                        <ul>
                            <li><strong>Early Fusion:</strong> When modalities are tightly coupled (audio-visual speech)</li>
                            <li><strong>Late Fusion:</strong> When modalities can operate independently (text + image classification)</li>
                            <li><strong>Hybrid Fusion:</strong> Complex multimodal understanding tasks (VQA, captioning)</li>
                            <li><strong>Cross-Attention:</strong> When one modality should guide processing of another</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>üëÅÔ∏è Vision-Language Models Deep Dive</h2>
                    <div class="vl-models">
                        <div class="model">
                            <h3>CLIP (OpenAI, 2021)</h3>
                            <p>Contrastive Language-Image Pre-training</p>
                            <ul>
                                <li><strong>Architecture:</strong> Dual encoder (ViT/ResNet + Transformer)</li>
                                <li><strong>Training:</strong> 400M image-text pairs from web</li>
                                <li><strong>Loss:</strong> Symmetric cross-entropy on similarity matrix</li>
                                <li><strong>Capabilities:</strong> Zero-shot classification, retrieval</li>
                                <li><strong>Impact:</strong> Foundation for most multimodal systems</li>
                            </ul>
                            <div class="formula-small">
                                <code>L = -log(exp(sim(I,T)/œÑ) / Œ£exp(sim(I,T')/œÑ))</code>
                            </div>
                        </div>
                        <div class="model">
                            <h3>DALLE-2 (OpenAI, 2022)</h3>
                            <p>Hierarchical text-to-image generation</p>
                            <ul>
                                <li><strong>Architecture:</strong> CLIP + Prior + Diffusion Decoder</li>
                                <li><strong>Two-stage:</strong> Text‚ÜíCLIP image embedding‚ÜíImage</li>
                                <li><strong>Prior:</strong> Transformer maps text to image embeddings</li>
                                <li><strong>Decoder:</strong> Diffusion upsampler (64√ó64‚Üí1024√ó1024)</li>
                                <li><strong>Quality:</strong> Photorealistic, coherent compositions</li>
                            </ul>
                        </div>
                        <div class="model">
                            <h3>Flamingo (DeepMind, 2022)</h3>
                            <p>Few-shot learning across vision and language</p>
                            <ul>
                                <li><strong>Architecture:</strong> Frozen LLM + Vision encoder + Cross-attention</li>
                                <li><strong>Perceiver Resampler:</strong> Variable vision inputs ‚Üí fixed tokens</li>
                                <li><strong>Cross-attention layers:</strong> Interleaved with LLM self-attention</li>
                                <li><strong>In-context learning:</strong> Few-shot multimodal tasks</li>
                                <li><strong>Flexibility:</strong> Handles interleaved image-text sequences</li>
                            </ul>
                        </div>
                    </div>
                    
                    <div class="vl-models">
                        <div class="model">
                            <h3>BLIP/BLIP-2 (Salesforce)</h3>
                            <p>Bootstrapped vision-language understanding</p>
                            <ul>
                                <li><strong>Key innovation:</strong> Bootstrap caption generation and filtering</li>
                                <li><strong>Multi-task:</strong> Captioning, VQA, retrieval in single model</li>
                                <li><strong>BLIP-2:</strong> Frozen LLM + Q-Former bridge module</li>
                                <li><strong>Efficiency:</strong> Minimal trainable parameters</li>
                            </ul>
                        </div>
                        <div class="model">
                            <h3>LLaVA (UW/Microsoft)</h3>
                            <p>Large Language and Vision Assistant</p>
                            <ul>
                                <li><strong>Simple approach:</strong> Vision encoder + projection + LLM</li>
                                <li><strong>Instruction tuning:</strong> GPT-4 generated conversation data</li>
                                <li><strong>Strong performance:</strong> Matches complex architectures</li>
                                <li><strong>Open source:</strong> Democratized multimodal capabilities</li>
                            </ul>
                        </div>
                        <div class="model">
                            <h3>GPT-4V (OpenAI, 2023)</h3>
                            <p>Multimodal GPT-4 with vision capabilities</p>
                            <ul>
                                <li><strong>Architecture:</strong> Unknown, likely integrated vision-text</li>
                                <li><strong>Capabilities:</strong> Complex reasoning over images</li>
                                <li><strong>Performance:</strong> SOTA on most vision-language benchmarks</li>
                                <li><strong>Integration:</strong> Seamless text and vision processing</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üéØ Cross-Modal Attention Deep Dive</h2>
                    <div class="cross-attention-explanation">
                        <h3>Mathematical Foundation</h3>
                        <div class="formula">
                            <code>CrossAttention(Q<sub>text</sub>, K<sub>vision</sub>, V<sub>vision</sub>) = softmax(Q<sub>text</sub>K<sub>vision</sub><sup>T</sup>/‚àöd)V<sub>vision</sub></code>
                        </div>
                        
                        <h3>Information Flow Visualization</h3>
                        <div class="attention-flow">
                            <div class="modality">
                                <h4>Text Query</h4>
                                <p>"A red car driving on a mountain road"</p>
                            </div>
                            <div class="arrow">‚Üí</div>
                            <div class="attention-mechanism">
                                <h4>Cross-Attention</h4>
                                <p>Which image regions correspond to each word?</p>
                                <ul style="font-size: 0.8em; text-align: left;">
                                    <li>"red" ‚Üí car pixels</li>
                                    <li>"car" ‚Üí vehicle shape</li>
                                    <li>"mountain" ‚Üí background terrain</li>
                                    <li>"road" ‚Üí surface underneath</li>
                                </ul>
                            </div>
                            <div class="arrow">‚Üí</div>
                            <div class="modality">
                                <h4>Vision Keys/Values</h4>
                                <p>Patch embeddings from image</p>
                            </div>
                        </div>
                        
                        <h3>Bidirectional Cross-Attention</h3>
                        <div class="technical-details">
                            <h4>Vision ‚Üí Text:</h4>
                            <ul>
                                <li><strong>Query:</strong> Image patch embeddings</li>
                                <li><strong>Key/Value:</strong> Text token embeddings</li>
                                <li><strong>Use case:</strong> Image captioning, visual grounding</li>
                            </ul>
                            <h4>Text ‚Üí Vision:</h4>
                            <ul>
                                <li><strong>Query:</strong> Text token embeddings</li>
                                <li><strong>Key/Value:</strong> Image patch embeddings</li>
                                <li><strong>Use case:</strong> Visual question answering, text-guided image editing</li>
                            </ul>
                        </div>
                        
                        <h3>Implementation Patterns</h3>
                        <div class="attention-types">
                            <div class="type-box">
                                <h4>Sparse Cross-Attention</h4>
                                <p>Only attend to relevant regions/tokens to reduce computation</p>
                            </div>
                            <div class="type-box">
                                <h4>Hierarchical Attention</h4>
                                <p>Multi-scale attention from coarse to fine-grained features</p>
                            </div>
                            <div class="type-box">
                                <h4>Temporal Cross-Attention</h4>
                                <p>Attention across time for video-text understanding</p>
                            </div>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üìä Evaluation Challenges & Metrics Deep Dive</h2>
                    <div class="evaluation-challenges">
                        <div class="challenge">
                            <h3>Alignment & Semantic Metrics</h3>
                            <ul>
                                <li><strong>CLIP Score:</strong> cos(I<sub>emb</sub>, T<sub>emb</sub>), measures semantic alignment</li>
                                <li><strong>BLIP Score:</strong> Bidirectional image-text matching</li>
                                <li><strong>CLIPScore:</strong> Harmonic mean of precision and recall</li>
                                <li><strong>FID:</strong> Fr√©chet distance between real and generated image distributions</li>
                                <li><strong>Human evaluation:</strong> Amazon MTurk, expert annotation</li>
                            </ul>
                            <div class="technical-details">
                                <h4>Limitations:</h4>
                                <ul>
                                    <li>CLIP bias toward specific image/text distributions</li>
                                    <li>Difficulty capturing fine-grained alignment</li>
                                    <li>Human evaluation cost and subjectivity</li>
                                </ul>
                            </div>
                        </div>
                        <div class="challenge">
                            <h3>Compositional Understanding</h3>
                            <ul>
                                <li><strong>Winoground:</strong> Text-image compositional reasoning</li>
                                <li><strong>ARO:</strong> Attribution, Relation, Order understanding</li>
                                <li><strong>VQA v2:</strong> Visual question answering with bias reduction</li>
                                <li><strong>CLEVR:</strong> Compositional visual reasoning</li>
                                <li><strong>Counting benchmarks:</strong> Object counting accuracy</li>
                            </ul>
                            <div class="technical-details">
                                <h4>Key Challenges:</h4>
                                <ul>
                                    <li>Object relationships and spatial reasoning</li>
                                    <li>Attribute binding ("red car" vs "blue car")</li>
                                    <li>Negation understanding ("not a cat")</li>
                                </ul>
                            </div>
                        </div>
                        <div class="challenge">
                            <h3>Robustness & Fairness</h3>
                            <ul>
                                <li><strong>Adversarial examples:</strong> Targeted perturbations</li>
                                <li><strong>Domain shift:</strong> Performance across different datasets</li>
                                <li><strong>Bias evaluation:</strong> Gender, race, age representation</li>
                                <li><strong>Hallucination detection:</strong> Generated vs. real content discrimination</li>
                                <li><strong>Safety filters:</strong> NSFW, harmful content detection</li>
                            </ul>
                            <div class="technical-details">
                                <h4>Evaluation Frameworks:</h4>
                                <ul>
                                    <li>WinoBias for gender stereotypes</li>
                                    <li>FairFace for racial bias in face generation</li>
                                    <li>PASTA for stereotype amplification</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    
                    <div class="technical-details">
                        <h4>Emerging Evaluation Paradigms:</h4>
                        <ul>
                            <li><strong>Human-AI collaboration metrics:</strong> How well humans and AI systems work together</li>
                            <li><strong>Multi-step reasoning:</strong> Chain-of-thought evaluation for complex tasks</li>
                            <li><strong>Real-world deployment metrics:</strong> User engagement, task completion rates</li>
                            <li><strong>Efficiency metrics:</strong> FLOPs per quality unit, energy consumption</li>
                        </ul>
                    </div>
                </section>
            </section>

            <!-- Advanced Topics -->
            <section>
                <section data-background-color="#dc2626">
                    <h1>üöÄ Advanced Topics</h1>
                    <p>Cutting-edge research directions</p>
                </section>

                <section>
                    <h2>üé® Advanced Diffusion Techniques & Control</h2>
                    <div class="advanced-techniques">
                        <div class="technique">
                            <h3>ControlNet (2023)</h3>
                            <p>Spatial conditioning for precise control</p>
                            <ul>
                                <li><strong>Architecture:</strong> Trainable copy of U-Net encoder</li>
                                <li><strong>Conditions:</strong> Canny edges, depth, pose, segmentation</li>
                                <li><strong>Zero convolutions:</strong> Preserve original model weights</li>
                                <li><strong>Training:</strong> 50K Stable Diffusion steps on conditioned data</li>
                                <li><strong>Impact:</strong> Professional creative workflows</li>
                            </ul>
                            <div class="formula-small">
                                <code>y<sub>c</sub> = ControlNet(x, c) + U-Net(x)</code>
                            </div>
                        </div>
                        <div class="technique">
                            <h3>LoRA for Diffusion (2023)</h3>
                            <p>Efficient fine-tuning and style adaptation</p>
                            <ul>
                                <li><strong>Low-rank adaptation:</strong> W + AB, where A‚ààR<sup>d√ór</sup>, B‚ààR<sup>r√ók</sup></li>
                                <li><strong>Typical rank:</strong> r=16-64 for 1-5% parameters</li>
                                <li><strong>Applications:</strong> Style transfer, concept learning, character consistency</li>
                                <li><strong>Composability:</strong> Multiple LoRAs can be combined</li>
                                <li><strong>Community impact:</strong> Democratized model customization</li>
                            </ul>
                        </div>
                        <div class="technique">
                            <h3>IP-Adapter (2023)</h3>
                            <p>Image prompt conditioning without retraining</p>
                            <ul>
                                <li><strong>Decoupled cross-attention:</strong> Separate for text and image prompts</li>
                                <li><strong>CLIP image encoder:</strong> Extract visual semantics</li>
                                <li><strong>Adapter layers:</strong> Learn to condition on image features</li>
                                <li><strong>Plug-and-play:</strong> Works with existing Stable Diffusion models</li>
                                <li><strong>Use cases:</strong> Style reference, character consistency, image variations</li>
                            </ul>
                        </div>
                    </div>
                    
                    <div class="advanced-techniques">
                        <div class="technique">
                            <h3>InstructPix2Pix (2023)</h3>
                            <p>Instruction-based image editing</p>
                            <ul>
                                <li><strong>Training data:</strong> GPT-3.5 + Prompt-to-Prompt generated pairs</li>
                                <li><strong>Conditioning:</strong> Original image + text instruction</li>
                                <li><strong>Architecture:</strong> Modified Stable Diffusion with extra input channels</li>
                                <li><strong>Capability:</strong> Natural language image editing</li>
                            </ul>
                        </div>
                        <div class="technique">
                            <h3>DreamBooth (2022)</h3>
                            <p>Subject-driven fine-tuning</p>
                            <ul>
                                <li><strong>Personalization:</strong> 3-5 images of subject</li>
                                <li><strong>Class-specific prior:</strong> Prevent language drift</li>
                                <li><strong>Rare token identifier:</strong> Unique subject binding</li>
                                <li><strong>Applications:</strong> Personalized art, product visualization</li>
                            </ul>
                        </div>
                        <div class="technique">
                            <h3>Score Distillation (2022)</h3>
                            <p>3D generation via 2D diffusion guidance</p>
                            <ul>
                                <li><strong>DreamFusion:</strong> Text-to-3D via score distillation sampling</li>
                                <li><strong>Neural radiance fields:</strong> 3D representation learning</li>
                                <li><strong>Multi-view consistency:</strong> Viewpoint augmentation</li>
                                <li><strong>Applications:</strong> 3D asset creation, metaverse content</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üé≠ Multimodal AI Frontiers & Research Directions</h2>
                    <div class="frontiers-grid">
                        <div class="frontier">
                            <h3>Audio-Visual Intelligence</h3>
                            <ul>
                                <li><strong>Speech-driven animation:</strong> Realistic facial motion from audio</li>
                                <li><strong>Music visualization:</strong> Real-time visual generation from audio</li>
                                <li><strong>Sound localization:</strong> Spatial audio understanding in video</li>
                                <li><strong>Cross-modal retrieval:</strong> Find videos using audio descriptions</li>
                                <li><strong>Audiovisual speech recognition:</strong> Lip-reading + audio fusion</li>
                            </ul>
                            <div class="technical-details">
                                <h4>Key Models:</h4>
                                <ul>
                                    <li>Wav2Lip, MakeItTalk (speech animation)</li>
                                    <li>SoundNet, AudioCLIP (audio-visual learning)</li>
                                    <li>SpecVQGAN (audio generation)</li>
                                </ul>
                            </div>
                        </div>
                        <div class="frontier">
                            <h3>3D Spatial Understanding</h3>
                            <ul>
                                <li><strong>NeRF integration:</strong> Neural radiance fields for 3D scenes</li>
                                <li><strong>3D scene graphs:</strong> Structured 3D understanding</li>
                                <li><strong>Spatial reasoning:</strong> Object relationships in 3D space</li>
                                <li><strong>Point cloud processing:</strong> Direct 3D data understanding</li>
                                <li><strong>Embodied AI:</strong> Navigation and manipulation in 3D</li>
                            </ul>
                            <div class="technical-details">
                                <h4>Emerging Approaches:</h4>
                                <ul>
                                    <li>3D-CLIP for 3D-text alignment</li>
                                    <li>Point-BERT for 3D point clouds</li>
                                    <li>Neural Scene Graphs</li>
                                </ul>
                            </div>
                        </div>
                        <div class="frontier">
                            <h3>Temporal Multimodal Modeling</h3>
                            <ul>
                                <li><strong>Video generation:</strong> Temporal consistency and long sequences</li>
                                <li><strong>Video understanding:</strong> Activity recognition and description</li>
                                <li><strong>Temporal grounding:</strong> Locating events in time</li>
                                <li><strong>Dynamic scene modeling:</strong> Change detection and prediction</li>
                                <li><strong>Memory mechanisms:</strong> Long-term context retention</li>
                            </ul>
                            <div class="technical-details">
                                <h4>Technical Challenges:</h4>
                                <ul>
                                    <li>Computational complexity of attention over time</li>
                                    <li>Temporal consistency in generation</li>
                                    <li>Long-range dependency modeling</li>
                                </ul>
                            </div>
                        </div>
                        <div class="frontier">
                            <h3>Interactive & Embodied AI</h3>
                            <ul>
                                <li><strong>Conversational agents:</strong> Multi-turn multimodal dialogue</li>
                                <li><strong>Real-time interaction:</strong> Low-latency multimodal processing</li>
                                <li><strong>Embodied agents:</strong> Physical world interaction</li>
                                <li><strong>Human-AI collaboration:</strong> Seamless multimodal interfaces</li>
                                <li><strong>Adaptive interfaces:</strong> Context-aware multimodal UX</li>
                            </ul>
                            <div class="technical-details">
                                <h4>Applications:</h4>
                                <ul>
                                    <li>Smart assistants (Alexa, Siri next-gen)</li>
                                    <li>Robotic assistants and companions</li>
                                    <li>AR/VR immersive experiences</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    
                    <div class="technical-details">
                        <h4>Cross-Cutting Research Themes:</h4>
                        <ul>
                            <li><strong>Foundation models:</strong> Large-scale multimodal pre-training</li>
                            <li><strong>Efficient architectures:</strong> Mobile and edge deployment</li>
                            <li><strong>Few-shot learning:</strong> Rapid adaptation to new tasks</li>
                            <li><strong>Causal reasoning:</strong> Understanding cause-effect in multimodal data</li>
                            <li><strong>Continual learning:</strong> Learning new modalities without forgetting</li>
                            <li><strong>Interpretability:</strong> Understanding multimodal decision-making</li>
                        </ul>
                    </div>
                </section>
            </section>

            <!-- Interview Questions -->
            <section>
                <section data-background-color="#059669">
                    <h1>‚ùì Interview Deep Dives</h1>
                    <p>Technical questions you might encounter</p>
                </section>

                <section>
                    <h2>üåä Diffusion Model Deep Technical Questions</h2>
                    <div class="interview-qa">
                        <h3 class="question">Q: "Explain the training process of diffusion models and why the simple L2 loss works"</h3>
                        <div class="answer">
                            <h4>Training Process:</h4>
                            <ol>
                                <li><strong>Sample timestep t</strong> uniformly from [1, T] (typically T=1000)</li>
                                <li><strong>Sample noise Œµ</strong> from N(0, I)</li>
                                <li><strong>Create noisy image:</strong> x<sub>t</sub> = ‚àö·æ±<sub>t</sub> ¬∑ x<sub>0</sub> + ‚àö(1-·æ±<sub>t</sub>) ¬∑ Œµ</li>
                                <li><strong>Predict noise:</strong> Œµ<sub>Œ∏</sub>(x<sub>t</sub>, t) using U-Net</li>
                                <li><strong>Compute loss:</strong> L = ||Œµ - Œµ<sub>Œ∏</sub>(x<sub>t</sub>, t)||¬≤</li>
                                <li><strong>Backpropagate:</strong> Update model parameters Œ∏</li>
                            </ol>
                            <h4>Why L2 Loss Works:</h4>
                            <ul>
                                <li><strong>Theoretical foundation:</strong> Equivalent to denoising score matching</li>
                                <li><strong>Score function connection:</strong> ‚àá<sub>x</sub> log p(x<sub>t</sub>) = -Œµ/œÉ<sub>t</sub></li>
                                <li><strong>Simplicity advantage:</strong> Avoids complex ELBO optimization</li>
                                <li><strong>Empirical success:</strong> Better results than variational lower bound</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üé≠ Multimodal Architecture Questions</h2>
                    <div class="interview-qa">
                        <h3 class="question">Q: "How would you handle modality imbalance and missing modalities?"</h3>
                        <div class="answer">
                            <h4>Modality Imbalance Solutions:</h4>
                            <ul>
                                <li><strong>Balanced sampling:</strong> Ensure equal modality representation per batch</li>
                                <li><strong>Loss reweighting:</strong> Œ±¬∑L<sub>text</sub> + Œ≤¬∑L<sub>vision</sub> with learned or tuned weights</li>
                                <li><strong>Curriculum learning:</strong> Start with single modalities, gradually add complexity</li>
                                <li><strong>Modality dropout:</strong> Randomly mask modalities during training (10-20%)</li>
                                <li><strong>Separate learning rates:</strong> Different optimization schedules per modality</li>
                            </ul>
                            <h4>Missing Modality Handling:</h4>
                            <ul>
                                <li><strong>Mask tokens:</strong> Learnable embeddings for missing inputs</li>
                                <li><strong>Zero padding:</strong> With appropriate attention masking</li>
                                <li><strong>Hallucination networks:</strong> Generate missing modality from available ones</li>
                                <li><strong>Uncertainty modeling:</strong> Bayesian approaches for missing data</li>
                                <li><strong>Robust architectures:</strong> Design for graceful degradation</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üìê Mathematical Deep Dive Questions</h2>
                    <div class="interview-qa">
                        <h3 class="question">Q: "Derive the DDIM sampling formula from DDPM"</h3>
                        <div class="answer">
                            <h4>Starting from DDPM reverse process:</h4>
                            <div class="formula-small">
                                <code>x<sub>t-1</sub> = Œº<sub>t</sub>(x<sub>t</sub>, x<sub>0</sub>) + œÉ<sub>t</sub>z</code>
                            </div>
                            <h4>Key insight: Parameterize in terms of x<sub>0</sub>:</h4>
                            <div class="formula-small">
                                <code>x<sub>t</sub> = ‚àö·æ±<sub>t</sub>x<sub>0</sub> + ‚àö(1-·æ±<sub>t</sub>)Œµ</code>
                                <code>‚üπ x<sub>0</sub> = (x<sub>t</sub> - ‚àö(1-·æ±<sub>t</sub>)Œµ<sub>Œ∏</sub>)/‚àö·æ±<sub>t</sub></code>
                            </div>
                            <h4>DDIM formula derivation:</h4>
                            <div class="formula-small">
                                <code>x<sub>t-1</sub> = ‚àö·æ±<sub>t-1</sub> ¬∑ xÃÇ<sub>0</sub> + ‚àö(1-·æ±<sub>t-1</sub>-œÉ<sub>t</sub>¬≤) ¬∑ Œµ<sub>Œ∏</sub> + œÉ<sub>t</sub>z</code>
                            </div>
                            <p><strong>Where œÉ<sub>t</sub> = 0 gives deterministic sampling</strong></p>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üöÄ System Design & Optimization Questions</h2>
                    <div class="interview-qa">
                        <h3 class="question">Q: "Design a real-time multimodal AI system for live video understanding"</h3>
                        <div class="answer">
                            <h4>Architecture Components:</h4>
                            <ul>
                                <li><strong>Video encoder:</strong> EfficientNet or MobileViT for mobile deployment</li>
                                <li><strong>Audio encoder:</strong> Wav2Vec 2.0 or Whisper for speech/sound</li>
                                <li><strong>Text processor:</strong> Lightweight BERT or DistilBERT</li>
                                <li><strong>Temporal modeling:</strong> Sliding window attention (16-32 frames)</li>
                                <li><strong>Fusion layer:</strong> Cross-attention with linear complexity</li>
                                <li><strong>Output heads:</strong> Task-specific heads for classification/QA</li>
                            </ul>
                            <h4>Real-time Optimization Strategies:</h4>
                            <ul>
                                <li><strong>Model quantization:</strong> INT8 inference for 4x speedup</li>
                                <li><strong>Temporal chunking:</strong> Process 2-4 second segments</li>
                                <li><strong>Feature caching:</strong> Cache intermediate representations</li>
                                <li><strong>Adaptive inference:</strong> Early exit based on confidence</li>
                                <li><strong>Pipeline parallelism:</strong> Overlap processing stages</li>
                                <li><strong>Dynamic batching:</strong> Batch multiple streams when possible</li>
                            </ul>
                            <h4>Infrastructure Considerations:</h4>
                            <ul>
                                <li><strong>Edge deployment:</strong> TensorRT, CoreML optimization</li>
                                <li><strong>Load balancing:</strong> Distribute across multiple GPUs/edge devices</li>
                                <li><strong>Fallback mechanisms:</strong> Graceful degradation on hardware failure</li>
                                <li><strong>Monitoring:</strong> Latency, throughput, accuracy metrics</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üî¨ Research & Innovation Questions</h2>
                    <div class="interview-qa">
                        <h3 class="question">Q: "What are the current limitations of diffusion models and how would you address them?"</h3>
                        <div class="answer">
                            <h4>Current Limitations:</h4>
                            <ul>
                                <li><strong>Slow sampling:</strong> 50-1000 steps vs single forward pass</li>
                                <li><strong>Mode collapse:</strong> Less diverse than GANs in some domains</li>
                                <li><strong>Training instability:</strong> Sensitive to hyperparameters</li>
                                <li><strong>Limited controllability:</strong> Difficult fine-grained control</li>
                                <li><strong>Memory requirements:</strong> Large U-Net models</li>
                            </ul>
                            <h4>Proposed Solutions:</h4>
                            <ul>
                                <li><strong>Fast sampling:</strong> Consistency models, progressive distillation</li>
                                <li><strong>Better architectures:</strong> DiT (Diffusion Transformers), scalable designs</li>
                                <li><strong>Improved training:</strong> Flow matching, rectified flow</li>
                                <li><strong>Enhanced control:</strong> ControlNet, IP-Adapter variations</li>
                                <li><strong>Efficiency:</strong> Latent diffusion, progressive training</li>
                            </ul>
                        </div>
                    </div>
                </section>
            </section>

            <!-- Next Steps -->
            <section data-background-gradient="linear-gradient(45deg, #059669, #7c3aed)">
                <h1>üéØ Mastery Checklist</h1>
                <div class="mastery-checklist">
                    <h2>Diffusion Models</h2>
                    <ul>
                        <li>‚úÖ Understand forward/reverse processes</li>
                        <li>‚úÖ Know sampling trade-offs</li>
                        <li>‚úÖ Familiar with conditioning techniques</li>
                        <li>‚úÖ Can explain training objective</li>
                    </ul>
                    
                    <h2>Multimodal AI</h2>
                    <ul>
                        <li>‚úÖ Understand fusion strategies</li>
                        <li>‚úÖ Know major VL models</li>
                        <li>‚úÖ Familiar with evaluation metrics</li>
                        <li>‚úÖ Can design multimodal systems</li>
                    </ul>
                </div>
                <p><a href="index.html" style="color: #ff6b35;">‚Üê Back to Main Slideshow</a></p>
            </section>
        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/reveal.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/notes/notes.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/markdown/markdown.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/plugin/highlight/highlight.min.js"></script>
    <script src="script.js"></script>
</body>
</html>
